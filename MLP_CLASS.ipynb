{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Hm6qgy1Q2G5r"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "\n",
        "  def __init__( self, sizes):\n",
        "    self.S = sizes\n",
        "    self.L = len(self.S)\n",
        "    self.Y = []\n",
        "    self.W = [0]\n",
        "    self.dW = [0]\n",
        "  \n",
        "  def bias_add(self,V):\n",
        "    bias = -np.ones( (len(V),1) )\n",
        "    return np.concatenate( (V,bias), axis=1)\n",
        "\n",
        "  def bias_sub(self,V):\n",
        "    return V[:,:-1]\n",
        "\n",
        "  def activation(self,X):\n",
        "    \n",
        "    for k in range(1,self.L-1):\n",
        "        self.Y[k][:] = self.bias_add( np.tanh( np.dot(self.Y[k-1],self.W[k]) ) )\n",
        "        \n",
        "    self.Y[-1][:] = np.tanh( np.dot(self.Y[-2],self.W[-1]) )\n",
        "    #return self.Y\n",
        "\n",
        "  def correction(self,Z,lr):\n",
        "    self.dW = [0]\n",
        "    for i in range(1,self.L): \n",
        "        self.dW.append( np.zeros( (self.S[i-1]+1,self.S[i]) ) )\n",
        "         \n",
        "    E = [0]*self.L # size L \n",
        "    D = [0]*self.L # size L\n",
        "    d = [0]*self.L # size L\n",
        "    \n",
        "    E[self.L - 1] =  Z - self.Y[-1] \n",
        "    d[self.L - 1] = 1 - self.Y[-1]**2\n",
        "    D[self.L - 1] = E[-1]*d[-1] \n",
        "        \n",
        "    for k in reversed(np.arange(1,self.L)):\n",
        "        self.dW[k] = lr * np.dot( self.Y[k-1].T, D[k]) # lr = learning rate (se le pasa a train)\n",
        "        E[k] = np.dot( D[k], self.W[k].T )\n",
        "        d[k] = ( 1 - self.Y[k-1]**2 )\n",
        "        D[k-1] = self.bias_sub( (E[k]*d[k]) )\n",
        "    #return self.dW\n",
        "  \n",
        "  def adaptation(self):\n",
        "    \n",
        "    for k in range(1,self.L-1):\n",
        "      self.W[k] += self.dW[k]\n",
        "    #return self.W\n",
        "\n",
        "  def estimation(self,Z):\n",
        "\n",
        "    est = np.mean( np.square(Z - self.Y[-1]) )\n",
        "    return est\n",
        "\n",
        "  def train(self,X, Z, lr, eps, epoch):\n",
        "\n",
        "    i = 0\n",
        "    P = len(X)\n",
        "    for s in self.S: # Armo las capas\n",
        "        if i == self.L - 1:\n",
        "            self.Y.append( np.zeros((P,s)) )\n",
        "            i += 1\n",
        "        else:\n",
        "            self.Y.append( np.zeros((P,s+1)) )\n",
        "            i += 1\n",
        "        \n",
        "    self.Y[0][:] = self.bias_add(X)\n",
        "\n",
        "    for i in range(0,self.L-1): # Armo los W\n",
        "      w_i = np.random.normal( 0, 0.1, (self.S[i]+1,self.S[i+1]) )\n",
        "      self.W.append(w_i)\n",
        "\n",
        "    error = 1\n",
        "    iter = 0\n",
        "    errores = []\n",
        "    while error > eps and iter < epoch:\n",
        "\n",
        "      self.activation(X)\n",
        "      self.correction(Z,lr)\n",
        "      self.adaptation()\n",
        "      error = self.estimation(Z)\n",
        "\n",
        "      errores.append(error)\n",
        "      iter += 1\n",
        "\n",
        "    return errores"
      ],
      "metadata": {
        "id": "Xa6vr33xwzbA"
      },
      "execution_count": 84,
      "outputs": []
    }
  ]
}